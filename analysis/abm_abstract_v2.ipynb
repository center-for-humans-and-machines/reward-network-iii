{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4accfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Vectorized learning on fixed pool\n",
    "# -----------------------------\n",
    "\n",
    "def simulate_rewards_fixed_pool(depths, T, r0, r1, c, env_tau, rng, low_effort_step=True):\n",
    "    \"\"\"\n",
    "    Vectorized lifetime rewards for agents interacting with a fixed pool of environments.\n",
    "    Agents sample an environment index uniformly at start and after each failed block.\n",
    "    Tracks per-agent:\n",
    "      - env_idx: current environment id (int)\n",
    "      - invested: attempts spent on the current environment (int)\n",
    "    \"\"\"\n",
    "    N = depths.shape[0]\n",
    "    M = env_tau.shape[0]\n",
    "\n",
    "    depths = depths.astype(int, copy=False)\n",
    "    remaining = np.full(N, T, dtype=int)\n",
    "    reward    = np.zeros(N, dtype=float)\n",
    "\n",
    "    # per-agent env state\n",
    "    env_idx  = rng.integers(0, M, size=N, endpoint=False)\n",
    "    invested = np.zeros(N, dtype=int)\n",
    "\n",
    "    # convenience: finite tau mask on the fly by indexing\n",
    "    while True:\n",
    "        active = remaining > 0\n",
    "        if not np.any(active):\n",
    "            break\n",
    "\n",
    "        can_invest = active & (invested < depths)\n",
    "        if not np.any(can_invest):\n",
    "            # switch environments for those who hit the quota without unlocking\n",
    "            need_switch = active & (invested >= depths)\n",
    "            if not np.any(need_switch):\n",
    "                break\n",
    "\n",
    "            if low_effort_step:\n",
    "                take_lo = need_switch & (remaining > 0)\n",
    "                reward[take_lo]   += r0\n",
    "                remaining[take_lo] -= 1\n",
    "\n",
    "            # reset env/attempts\n",
    "            invested[need_switch] = 0\n",
    "            env_idx[need_switch] = rng.integers(0, M, size=need_switch.sum(), endpoint=False)\n",
    "            continue\n",
    "\n",
    "        # one high-effort action for all who can invest\n",
    "        reward[can_invest]   -= c\n",
    "        remaining[can_invest] -= 1\n",
    "        invested[can_invest] += 1\n",
    "\n",
    "        # check unlocks: invested >= tau(current env) and tau finite\n",
    "        tau_now = env_tau[env_idx]          # shape (N,)\n",
    "        finite  = np.isfinite(tau_now)\n",
    "        unlock_now = can_invest & finite & (invested.astype(float) >= tau_now)\n",
    "\n",
    "        if np.any(unlock_now):\n",
    "            reward[unlock_now]   += r1 * remaining[unlock_now]\n",
    "            remaining[unlock_now] = 0\n",
    "            # they are done; env_idx/invested irrelevant after remaining=0\n",
    "\n",
    "        # those who reached depth without unlocking will be handled in next pass\n",
    "        # (when can_invest becomes False for them)\n",
    "\n",
    "    reward[reward < 0.0] = 0.0\n",
    "    return reward\n",
    "\n",
    "# -----------------------------\n",
    "# ABM wrapper with fixed pool\n",
    "# -----------------------------\n",
    "\n",
    "def reproduce_with_mutation(depths, fitness, m, d_max, rng, epsilon=1e-12):\n",
    "    N = len(depths)\n",
    "    weights = np.asarray(fitness, dtype=float) + epsilon\n",
    "    p = weights / weights.sum()\n",
    "    parents = rng.choice(N, size=N, replace=True, p=p)\n",
    "    new_depths = depths[parents].copy()\n",
    "\n",
    "    mutate = rng.random(N) < m\n",
    "    dirs = np.where(rng.random(N) < 0.5, -1, +1)\n",
    "    new_depths[mutate] += dirs[mutate]\n",
    "    # reflect at boundaries\n",
    "    new_depths[new_depths < 1] = 2 - new_depths[new_depths < 1]\n",
    "    new_depths[new_depths > d_max] = 2*d_max - new_depths[new_depths > d_max]\n",
    "    return new_depths\n",
    "\n",
    "def run_abm_fixed_pool(\n",
    "    generations, N, T,\n",
    "    r0, r1, c,\n",
    "    pi, D_spec,\n",
    "    m, d_max,\n",
    "    M_env=200_000,\n",
    "    seed=0,\n",
    "    init_depths=None,\n",
    "    record_hist=True\n",
    "):\n",
    "    \"\"\"\n",
    "    ABM with a fixed environment pool of size M_env created once at the beginning.\n",
    "    Agents sample environments uniformly (with replacement) at each switch.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Build fixed pool once\n",
    "    env_tau = build_environment_pool(M_env, pi, D_spec, seed=rng.integers(1<<31))\n",
    "\n",
    "    # Initialize depths\n",
    "    if init_depths is None:\n",
    "        depths = rng.integers(1, d_max+1, size=N)\n",
    "    else:\n",
    "        depths = np.asarray(init_depths, dtype=int).copy()\n",
    "        if depths.shape != (N,):\n",
    "            raise ValueError(\"init_depths must have shape (N,)\")\n",
    "\n",
    "    # History\n",
    "    history = None\n",
    "    if record_hist:\n",
    "        hist_mean = np.zeros(generations+1)\n",
    "        hist_var  = np.zeros(generations+1)\n",
    "        hist_dist = np.zeros((generations+1, d_max))\n",
    "        hist_mean[0] = depths.mean()\n",
    "        hist_var[0]  = depths.var()\n",
    "        counts = np.bincount(depths, minlength=d_max+1)[1:]\n",
    "        hist_dist[0] = counts / counts.sum()\n",
    "\n",
    "    # Generational loop\n",
    "    for g in range(1, generations+1):\n",
    "        fitness = simulate_rewards_fixed_pool(depths, T, r0, r1, c, env_tau, rng)\n",
    "\n",
    "        depths = reproduce_with_mutation(depths, fitness, m=m, d_max=d_max, rng=rng)\n",
    "\n",
    "        if record_hist:\n",
    "            hist_mean[g] = depths.mean()\n",
    "            hist_var[g]  = depths.var()\n",
    "            counts = np.bincount(depths, minlength=d_max+1)[1:]\n",
    "            hist_dist[g] = counts / counts.sum()\n",
    "\n",
    "    if record_hist:\n",
    "        history = {\"mean_depth\": hist_mean,\n",
    "                   \"var_depth\": hist_var,\n",
    "                   \"depth_distribution\": hist_dist}\n",
    "    return depths, history\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    generations = 200\n",
    "    N = 5000\n",
    "    T = 40\n",
    "    r0, r1, c = 1.0, 6.0, 1.0\n",
    "    pi = 0.4\n",
    "    D_spec = {\"name\": \"mixture\",\n",
    "              \"alpha\": 0.2,\n",
    "              \"tau_easy\": 3,\n",
    "              \"hard\": {\"name\": \"geometric\", \"p\": 0.03}}\n",
    "    m = 0.05\n",
    "    d_max = 60\n",
    "    M_env = 300_000\n",
    "    seed = 123\n",
    "\n",
    "    depths, hist = run_abm_fixed_pool(\n",
    "        generations, N, T,\n",
    "        r0, r1, c, pi, D_spec,\n",
    "        m, d_max,\n",
    "        M_env=M_env,\n",
    "        seed=seed,\n",
    "        record_hist=True\n",
    "    )\n",
    "    print(\"Final mean depth:\", hist[\"mean_depth\"][-1])\n",
    "    print(\"Depth dist d=1..15:\", np.round(hist[\"depth_distribution\"][-1, :15], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cd3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Environment pool\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# Simple exponential distribution\n",
    "def get_tau(M, pt, rng):\n",
    "    return rng.exponential(scale=1/pt, size=M)\n",
    "\n",
    "\n",
    "def build_environment_pool(M, pi, pt, seed):\n",
    "    \"\"\"\n",
    "    Build a fixed pool of M environments once.\n",
    "    Each environment has tau in {1,2,...} if unlockable defining the number of hard learning attempts to unlock the environment; else tau = -1.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    M_unlockable = int(M*pi)\n",
    "    unlockable = np.zeros(M, dtype=bool)\n",
    "    unlockable[:M_unlockable] = True\n",
    "    tau_vals = get_tau(M_unlockable, pt, rng)\n",
    "    tau = np.full(M, -1, dtype=int)\n",
    "    tau[unlockable] = tau_vals\n",
    "    return tau  # shape (M,), -1 for non-unlockable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068dee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Vectorized learning on fixed pool\n",
    "# -----------------------------\n",
    "\n",
    "def simulate_rewards_fixed_pool(agent_depths, agent_persistence, agent_T, env_r0, env_r1, env_c, env_tau, rng, low_effort_step=True):\n",
    "    \"\"\"\n",
    "    Vectorized lifetime rewards for agents interacting with a fixed pool of environments.\n",
    "    Agents sample an environment index uniformly at start and after each failed block.\n",
    "    Tracks per-agent:\n",
    "      - env_idx: current environment id (int)\n",
    "      - invested: attempts spent on the current environment (int)\n",
    "    \"\"\"\n",
    "    N = agent_depths.shape[0] # number of agents\n",
    "    M = env_tau.shape[0] # number of environments\n",
    "\n",
    "    assert agent_depths.dtype == int\n",
    "    assert agent_T.dtype == int\n",
    "    assert env_r0.dtype == float\n",
    "    assert env_r1.dtype == float\n",
    "    assert env_c.dtype == float\n",
    "    assert env_tau.dtype == int\n",
    "\n",
    "    assert agent_T.shape == (N,)\n",
    "    remaining = agent_T.copy()\n",
    "    reward    = np.zeros(N, dtype=float)\n",
    "    exhausted = np.zeros(N, dtype=bool)\n",
    "    unlocked = np.zeros(N, dtype=bool)\n",
    "\n",
    "    # per-agent env state\n",
    "    env_idx  = rng.integers(0, M, size=N, endpoint=False)\n",
    "    invested = np.zeros(N, dtype=int)\n",
    "\n",
    "    # convenience: finite tau mask on the fly by indexing\n",
    "    while True:\n",
    "        active = remaining > 0\n",
    "        if not np.any(active):\n",
    "            break\n",
    "\n",
    "        # agents that can invest\n",
    "        can_invest = active & (invested < agent_depths)\n",
    "        \n",
    "        need_switch = active & (invested >= agent_depths)\n",
    "        might_exhaust = rng.random(N) < agent_persistence\n",
    "        will_exhaust = need_switch & might_exhaust\n",
    "        exhausted = exhausted | will_exhaust\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if not np.any(can_invest):\n",
    "            # switch environments for those who hit the quota without unlocking\n",
    "            need_switch = active & (invested >= depths)\n",
    "            if not np.any(need_switch):\n",
    "                break\n",
    "\n",
    "            if low_effort_step:\n",
    "                take_lo = need_switch & (remaining > 0)\n",
    "                reward[take_lo]   += r0\n",
    "                remaining[take_lo] -= 1\n",
    "\n",
    "            # reset env/attempts\n",
    "            invested[need_switch] = 0\n",
    "            env_idx[need_switch] = rng.integers(0, M, size=need_switch.sum(), endpoint=False)\n",
    "            continue\n",
    "\n",
    "        # one high-effort action for all who can invest\n",
    "        reward[can_invest]   -= c\n",
    "        remaining[can_invest] -= 1\n",
    "        invested[can_invest] += 1\n",
    "\n",
    "        # check unlocks: invested >= tau(current env) and tau finite\n",
    "        tau_now = env_tau[env_idx]          # shape (N,)\n",
    "        finite  = np.isfinite(tau_now)\n",
    "        unlock_now = can_invest & finite & (invested.astype(float) >= tau_now)\n",
    "\n",
    "        if np.any(unlock_now):\n",
    "            reward[unlock_now]   += r1 * remaining[unlock_now]\n",
    "            remaining[unlock_now] = 0\n",
    "            # they are done; env_idx/invested irrelevant after remaining=0\n",
    "\n",
    "        # those who reached depth without unlocking will be handled in next pass\n",
    "        # (when can_invest becomes False for them)\n",
    "\n",
    "    reward[reward < 0.0] = 0.0\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ec68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def interact_hidden_path(\n",
    "    depths,                    # (N,) int, per-agent exploration depth d_i\n",
    "    T,                         # int, per-agent lifetime budget (steps/attempts)\n",
    "    env_M,                     # (M_env,) int, fixed pool of environment difficulties M_e >= 1\n",
    "    theta,                     # float in (0,1), per-step \"right turn\" probability\n",
    "    rho,                       # float in [0,1], persistence: stay after failed attempt\n",
    "    r_min=1.0, alpha=0.2, beta=1.2,  # parameters for R(M) = r_min + alpha * M**beta\n",
    "    r_fail=0.0,\n",
    "    rng=None                   # np.random.Generator (optional)\n",
    "):\n",
    "    \"\"\"\n",
    "    Vectorized agentâ€“environment interaction for the active-reset hidden-path model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rewards : (N,) float\n",
    "        Realized cumulative reward per agent (non-negative).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    N = depths.shape[0]\n",
    "    M_env = env_M.shape[0]\n",
    "    depths = depths.astype(int, copy=False)\n",
    "    env_M = env_M.astype(int, copy=False)\n",
    "\n",
    "    # Reward mapping R(M)\n",
    "    def R_of_M(M):\n",
    "        val = r_min + alpha * np.power(M, beta)\n",
    "        return val\n",
    "\n",
    "    # State\n",
    "    remaining = np.full(N, int(T), dtype=int)\n",
    "    rewards   = np.zeros(N, dtype=float)\n",
    "\n",
    "    # Assign initial environments\n",
    "    env_idx = rng.integers(0, M_env, size=N, endpoint=False)\n",
    "\n",
    "    # Precompute theta^M for all envs (speeds up repeated masking)\n",
    "    theta_pow_all = np.power(theta, env_M)\n",
    "    \n",
    "\n",
    "    # Main event loop (attempt-wise, fully vectorized across agents)\n",
    "    while True:\n",
    "        active = remaining > 0\n",
    "        if not np.any(active):\n",
    "            break\n",
    "\n",
    "        # Effective attempt depth k_i = min(d_i, remaining_i)\n",
    "        k = np.minimum(depths, remaining)\n",
    "\n",
    "        # Per-attempt success probability: q = 1{M<=k} * theta^M\n",
    "        M_now   = env_M[env_idx]\n",
    "        feasible = (M_now <= k)\n",
    "        q = theta_pow_all[env_idx].copy()\n",
    "        q[~feasible] = 0.0\n",
    "\n",
    "        # Draw successes\n",
    "        u = rng.random(N)\n",
    "        success = (u < q) & active\n",
    "        fail = (~success) & active\n",
    "\n",
    "        # Steps spent this attempt\n",
    "        steps_spent = np.zeros(N, dtype=int)\n",
    "        steps_spent[success] = M_now[success]\n",
    "        steps_spent[fail] = k[fail]\n",
    "\n",
    "        # Decrement remaining\n",
    "        remaining -= steps_spent\n",
    "\n",
    "        # Handle successes: exploitation (optional) then always switch to a fresh environment if time remains\n",
    "        rewards[success] += R_of_M(M_now[success])\n",
    "        rewards[fail] += r_fail\n",
    "\n",
    "        # Stay/switch draws only for failing, still-active agents\n",
    "        switch = fail & (rng.random(N) > rho)\n",
    "\n",
    "        # After success, if budget remains, draw new environment\n",
    "        need_new_env = (switch | success) & (remaining > 0)\n",
    "\n",
    "        if np.any(need_new_env):\n",
    "            env_idx[need_new_env] = rng.integers(0, M_env, size=need_new_env.sum())\n",
    "\n",
    "    return rewards\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
